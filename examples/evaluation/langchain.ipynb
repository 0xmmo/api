{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install multion langchain beautifulsoup4  -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'multion.multion'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on https://127.0.0.1:8000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [09/Jul/2023 04:37:52] \"GET /callback?code=5b1bf846-d090-4cd3-8759-30cd39ba6163&state=XldfZUcRwuuGci5xEJ0zkvs02N1JlI HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import multion\n",
    "\n",
    "multion.login()\n",
    "#multion.refresh_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGCHAIN EXAMPLE\n",
    "from langchain import  OpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"<openai_api_key>\"\n",
    "\n",
    "\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class MultionAPI:\n",
    "    def __init__(self):\n",
    "        self.tabId = None\n",
    "        self.new_session_count = 0\n",
    "\n",
    "    def create_session(self, query: str, url: str) -> str:\n",
    "        \"\"\"Always the first step to run any activities that can be done using browser.\n",
    "        The function parameters 'query' and 'url' both are compulsary.\n",
    "        'query' is the query that you need to perform in the given url.if there is no 'query'set it as open.\n",
    "        'url' is the base url of a site.\"\"\"\n",
    "        # Only create new session once and continue using update session\n",
    "        if self.new_session_count < 2:\n",
    "            response = multion.new_session({\"input\": query,\"url\": url})\n",
    "            self.new_session_count +=1\n",
    "            self.tabId = response['tabId']\n",
    "            return  response['message']\n",
    "        else:\n",
    "            return \"Continue using update session\"\n",
    "\n",
    "    def update_session(self, query:str,url:str) -> str:\n",
    "        \"\"\"Updates the existing browser session created using create_session with given action and url, used consequently to handle browser activitites after creating one session of browser.\n",
    "        The function parameters 'query' and 'url' both are compulsary.\n",
    "        'query' is the query that you need to perform in the given url.if there is no 'query'set it as open.\n",
    "        'url' is the base url of a site.\"\"\"\n",
    "        response = multion.update_session(self.tabId, {\"input\": query,\"url\": url})\n",
    "        return  response['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "multion_api = MultionAPI()\n",
    "tool1 = StructuredTool.from_function(multion_api.create_session)\n",
    "tool2= StructuredTool.from_function(multion_api.update_session)\n",
    "\n",
    "llm = OpenAI(temperature=0,max_tokens=1000)\n",
    "\n",
    "# Structured tools are compatible with the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type.\n",
    "agent_executor = initialize_agent(\n",
    "    [tool1, tool2],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running post\n",
      "running post\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'look up the most recent rank for the most friendliest city in america',\n",
       " 'output': 'The most friendly city in America according to the search results is Cincinnati, Ohio, followed by Chicago, Nashville, Key West, Florida, and Tucson, Arizona.',\n",
       " 'intermediate_steps': [(AgentAction(tool='create_session', tool_input={'query': 'most friendly city in america', 'url': 'https://www.google.com/'}, log='Action:\\n```\\n{\\n  \"action\": \"create_session\",\\n  \"action_input\": {\\n    \"query\": \"most friendly city in america\",\\n    \"url\": \"https://www.google.com/\"\\n  }\\n}\\n```\\n'),\n",
       "   'I am searching for the most friendly city in America on Google.\\n\\n'),\n",
       "  (AgentAction(tool='update_session', tool_input={'query': 'most friendly city in america rank', 'url': 'https://www.google.com/'}, log=' I need to find the most recent rank\\nAction:\\n```\\n{\\n  \"action\": \"update_session\",\\n  \"action_input\": {\\n    \"query\": \"most friendly city in america rank\",\\n    \"url\": \"https://www.google.com/\"\\n  }\\n}\\n```\\n\\n'),\n",
       "   'The most friendly city in America according to the search results is Cincinnati, Ohio, followed by Chicago, Nashville, Key West, Florida, and Tucson, Arizona.\\n')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor(f\"look up the most recent rank for the most friendliest city in america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "from langchain.llms import OpenAI\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self):\n",
    "         self.questions=[]\n",
    "         self.outputs={\"predicted\":[],\"truth\":[]}\n",
    "         self.queries = {\"predicted\":[],\"truth\":[]}\n",
    "         self.scores= defaultdict(list)\n",
    "         self.templates = {\n",
    "              \n",
    "        \"query_evaluation\":\"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "                > Question: {question}\n",
    "\n",
    "                The query sequence you know you should be executing against the API is:\n",
    "\n",
    "                > Query Sequence: {truth_query}\n",
    "\n",
    "                Is the following predicted query sequence semantically the same (eg likely to produce the same answer)?\n",
    "\n",
    "                > Predicted Query Sequence: {predict_query}\n",
    "\n",
    "                Please give the Predicted Query Sequence a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "                > Explanation: Let's think step by step.\"\"\",\n",
    "\n",
    "          \"output_evaluation\": \"\"\"You are trying to answer the following question by querying an API:\n",
    "\n",
    "                    > Question: {question}\n",
    "\n",
    "                    The API returned a response of:\n",
    "\n",
    "                    > API result: {api_response}\n",
    "\n",
    "                    Your response to the user: {answer}\n",
    "\n",
    "                    Please evaluate the accuracy and utility of your response to the user's original question, conditioned on the information available.\n",
    "                    Give a letter grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
    "\n",
    "                    > Explanation: Let's think step by step.\"\"\",\n",
    "        \n",
    "            \"question_generation\": \"\"\"MultiOn is personal browser copilot that can use webapps like twitter for tweeting, google search to get data for topic. \n",
    "                    \n",
    "                    Imagine you're a new user trying to use MultiOn , and want to test its features {features} .\n",
    "                    \n",
    "                     What are 2 different requests you want MultiOn to perform ? Make sure the requests should be clear and fully closed,\n",
    "                      such that no more inputs is needed and they are independent from each request.Assume all accounts are already signed in. \n",
    "                     example request: Make a tweet tagging MULTION_AI\n",
    "                    Wants/Questions:\n",
    "                    1. \"\"\"}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def query_sequence(self,response):\n",
    "        list_sequence = []\n",
    "        str_sequence = \"\"\n",
    "        intermediate_steps = response[\"intermediate_steps\"]\n",
    "        for i  in range(len(intermediate_steps)):\n",
    "            step_obj = intermediate_steps[i]\n",
    "            tool_input = step_obj[0].tool_input\n",
    "            step_dict = {\"action\":step_obj[0].tool,\"query\":tool_input[\"query\"],\"url\":tool_input[\"url\"]}\n",
    "            list_sequence.append(step_dict)\n",
    "            str_sequence = str_sequence +f\"{i}. {list_sequence[i]} \\n\"\n",
    "        return list_sequence, str_sequence\n",
    "    \n",
    "\n",
    "    def generate_questions(self):    \n",
    "        prompt = PromptTemplate.from_template(self.templates[\"question_generation\"])\n",
    "\n",
    "        generation_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "        questions_ = generation_chain.run(features=\"\").split('\\n')\n",
    "        # Strip preceding numeric bullets\n",
    "        questions = [re.sub(r'^\\d+\\. ', '', q).strip() for q in questions_]\n",
    "        return questions\n",
    "    \n",
    "    def generate_dataset(self,file_name=\"generated_dataset.json\",questions=None,):\n",
    "        if(questions==None):\n",
    "            questions = self.generate_questions()  \n",
    "        # Data to be written\n",
    "        generated_dataset = {}\n",
    "        print(questions)\n",
    "        for question in questions:\n",
    "            response = agent_executor(question)\n",
    "            print(question,\"  --- \",response[\"output\"])\n",
    "            generated_dataset[question]= {\"query_sequence\":self.query_sequence(response),\"output\":response[\"output\"]}\n",
    "        # Serializing json\n",
    "        json_object = json.dumps(generated_dataset)\n",
    "\n",
    "        # Writing to sample.json\n",
    "        with open(file_name, \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "    \n",
    "    def parse_eval_results(self,results: List[str]) -> List[float]:\n",
    "        rubric = {\"A\": 1.0, \"B\": 0.75, \"C\": 0.5, \"D\": 0.25, \"F\": 0}\n",
    "        return [rubric[re.search(r\"Final Grade: (\\w+)\", res).group(1)] for res in results]\n",
    "\n",
    "    def evaluate_query(self,):\n",
    "        prompt = PromptTemplate.from_template(self.templates[\"query_evaluation\"])\n",
    "        eval_chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
    "        request_eval_results = []\n",
    "        for question, predict_query, truth_query in list(\n",
    "            zip(self.questions, self.queries[\"predicted\"], self.queries[\"truth\"])\n",
    "        ):\n",
    "            eval_output = eval_chain.run(\n",
    "                question=question,\n",
    "                truth_query=truth_query,\n",
    "                predict_query=predict_query,\n",
    "            )\n",
    "            request_eval_results.append(eval_output)\n",
    "        return self.parse_eval_results(request_eval_results)\n",
    "    \n",
    "    def evaluate_output(self):\n",
    "        prompt = PromptTemplate.from_template(self.templates[\"output_evaluation\"])\n",
    "        eval_chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
    "        output_eval_results = []\n",
    "        for question, predict_output, truth_output in list(\n",
    "            zip(self.questions, self.outputs[\"predicted\"], self.outputs[\"truth\"])\n",
    "        ):\n",
    "            eval_output = eval_chain.run(\n",
    "                question=question,\n",
    "                api_response=predict_output,\n",
    "                answer=truth_output,\n",
    "            )\n",
    "            output_eval_results.append(eval_output)\n",
    "        return self.parse_eval_results(output_eval_results)\n",
    "        \n",
    "    \n",
    "    def evaluate_dataset(self,file_name=\"dataset.json\"):\n",
    "        self.questions=[]\n",
    "        self.outputs={\"predicted\":[],\"truth\":[]}\n",
    "        self.queries = {\"predicted\":[],\"truth\":[]}\n",
    "        self.scores= defaultdict(list)\n",
    "\n",
    "        # Opening JSON file\n",
    "        f = open(file_name)\n",
    "        # returns JSON object as dict\n",
    "        data = json.load(f)\n",
    "\n",
    "        #Unload the serch_texts and truth_query_sequences\n",
    "        for question,response in data.items():\n",
    "            query = response[\"query_sequence\"]\n",
    "            output = response[\"output\"]\n",
    "            self.questions.append(question)\n",
    "            self.queries[\"truth\"].append(query)\n",
    "            self.outputs[\"truth\"].append(output)\n",
    "\n",
    "        #predict query sequences\n",
    "        failed_examples = []\n",
    "\n",
    "        for question in self.questions:\n",
    "            try:\n",
    "                response=agent_executor(question)\n",
    "                self.queries[\"predicted\"].append(self.query_sequence(response))\n",
    "                self.outputs[\"predicted\"].append(response[\"output\"])\n",
    "            except Exception as e:\n",
    "                failed_examples.append({\"q\": question, \"error\": e})\n",
    "                self.scores[\"completed\"].append(0.0)\n",
    "        self.scores[\"query_scores\"].extend(self.evaluate_query())\n",
    "        self.scores[\"output_scores\"].extend(self.evaluate_output())\n",
    "        \"\"\" Printing Format!! \"\"\"\n",
    "        header = \"{:<20}\\t{:<10}\\t{:<10}\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\n",
    "        print(header)\n",
    "        for metric, metric_scores in self.scores.items():\n",
    "            mean_scores = (\n",
    "                sum(metric_scores) / len(metric_scores)\n",
    "                if len(metric_scores) > 0\n",
    "                else float(\"nan\")\n",
    "            )\n",
    "            row = \"{:<20}\\t{:<10.2f}\\t{:<10.2f}\\t{:<10.2f}\".format(\n",
    "                metric, min(metric_scores), mean_scores, max(metric_scores)\n",
    "            )\n",
    "            print(row)\n",
    "\n",
    "        return self.scores,failed_examples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Search for \"Best restaurants in New York City\"', '2.  Post a tweet with the hashtag #MultiOn_AI']\n",
      "running post\n",
      "running post\n",
      "Search for \"Best restaurants in New York City\"   ---  Here is a list of the best restaurants in New York City according to Time Out: https://www.timeout.com/newyork/restaurants/best-restaurants-in-nyc\n",
      "running post\n",
      "running post\n",
      "2.  Post a tweet with the hashtag #MultiOn_AI   ---  I have posted the tweet with the hashtag #MultiOn_AI\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluation()\n",
    "eval.generate_dataset(file_name=\"generated_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "multion_api = MultionAPI()\n",
    "tool1 = StructuredTool.from_function(multion_api.create_session)\n",
    "tool2= StructuredTool.from_function(multion_api.update_session)\n",
    "\n",
    "llm = OpenAI(temperature=0,max_tokens=1000)\n",
    "\n",
    "# Structured tools are compatible with the STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION agent type.\n",
    "agent_executor = initialize_agent(\n",
    "    [tool1, tool2],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running post\n",
      "running post\n",
      "running post\n",
      "Metric              \tMin       \tMean      \tMax       \n",
      "query_scores        \t0.00      \t0.00      \t0.00      \n",
      "output_scores       \t0.00      \t0.50      \t1.00      \n"
     ]
    }
   ],
   "source": [
    "eval = Evaluation()\n",
    "scores,failed = eval.evaluate_dataset(file_name=\"generated_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'query_scores': [0, 0], 'output_scores': [1.0, 0]})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
